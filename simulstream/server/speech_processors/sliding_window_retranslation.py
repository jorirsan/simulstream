# Copyright 2025 FBK

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License

from abc import abstractmethod
from difflib import SequenceMatcher
from types import SimpleNamespace
from typing import List

import torch

from simulstream.server.speech_processors import SAMPLE_RATE
from simulstream.server.speech_processors.base import BaseSpeechProcessor, IncrementalOutput


class SlidingWindowRetranslator(BaseSpeechProcessor):

    def __init__(self, config: SimpleNamespace):
        super().__init__(config)
        self.window_len = self.config.window_len * SAMPLE_RATE
        self.matching_threshold = getattr(self.config, "matching_threshold", 0.1)
        self.override_on_failed_match = getattr(self.config, "override_on_failed_match", False)
        self.max_tokens_per_second = getattr(self.config, "max_tokens_per_second", 10)

    @abstractmethod
    def _tokens_to_string(self, tokens: List[str]) -> str:
        ...

    def _build_incremental_outputs(self, generated_tokens: List[str]) -> IncrementalOutput:
        """
        Deduplicates the output stream of overlapping windows using the algorithm introduced in
        `S. Sen, et al. 2025. "Simultaneous Translation for Unsegmented Input:
        A Sliding Window Approach" <https://arxiv.org/pdf/2210.09754>`_

        This algorithm is based on the longest matching substring between the current and previous
        window. We use tokens instead of string to match, though, as we have empirically observed
        that tokenization is mostly consistent across generations of the same word.
        """
        if self.text_history is None or len(self.text_history) == 0:
            self.text_history = generated_tokens
            generated_string = self._tokens_to_string(generated_tokens)
            return IncrementalOutput(
                new_tokens=generated_tokens,
                new_string=generated_string,
                deleted_tokens=[],
                deleted_string=""
            )
        seq_matcher = SequenceMatcher(None, self.text_history, generated_tokens, autojunk=False)
        longest_match = seq_matcher.find_longest_match()
        if longest_match.size >= self.matching_threshold * len(generated_tokens):
            new_tokens = generated_tokens[longest_match.b + longest_match.size:]
            deleted_tokens = self.text_history[longest_match.a + longest_match.size:]
            new_string = self._tokens_to_string(new_tokens)
            deleted_string = self._tokens_to_string(deleted_tokens)
            # we take the matching part and the last part of the generated string as part of
            # the history. Then we take from the history the tokens corresponding to the amount
            # generated in this step, to ensure we have a sufficiently wide window
            matching_and_last_tokens = generated_tokens[longest_match.b:]
            initial_discarded_tokens = len(generated_tokens) - len(matching_and_last_tokens)
            history_tokens_discarded = self.text_history[longest_match.a:]
            history_initial_tokens = len(self.text_history) - len(history_tokens_discarded)
            new_history_initial_tokens = self.text_history[
                max(history_initial_tokens - initial_discarded_tokens, 0):history_initial_tokens]
            self.text_history = new_history_initial_tokens + matching_and_last_tokens
        else:
            if self.override_on_failed_match:
                deleted_tokens = self.text_history
                deleted_string = self._tokens_to_string(self.text_history)
            else:
                deleted_tokens = []
                deleted_string = ""
            new_tokens = generated_tokens
            new_string = self._tokens_to_string(generated_tokens)
            self.text_history = generated_tokens
        return IncrementalOutput(
            new_tokens=new_tokens,
            new_string=new_string,
            deleted_tokens=deleted_tokens,
            deleted_string=deleted_string,
        )

    def _update_speech_history(
            self,
            new_speech: torch.Tensor,
            generated_tokens: List[str],
            new_output: IncrementalOutput) -> None:
        pass

    def _update_text_history(
            self,
            new_speech: torch.Tensor,
            generated_tokens: List[str],
            new_output: IncrementalOutput) -> None:
        pass
